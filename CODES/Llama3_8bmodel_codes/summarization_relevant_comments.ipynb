{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VCZS0rnOmQW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f259ad4a-0235-4a61-bbb1-33e3349ace08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import re"
      ],
      "metadata": {
        "id": "utnR66jQmXsT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draft_message(content, role='user'):\n",
        "    return {\n",
        "        \"role\": role,\n",
        "        \"content\": content,\n",
        "    }\n",
        "\n",
        "def truncate_content(content, max_length=4000):\n",
        "    return content[:max_length]\n",
        "\n",
        "def truncate_comments(comments, max_length=480):\n",
        "    truncated_comments = []\n",
        "    total_length = 0\n",
        "    for comment in comments:\n",
        "        comment_length = len(comment)\n",
        "        if total_length + comment_length <= max_length:\n",
        "            truncated_comments.append(comment)\n",
        "            total_length += comment_length\n",
        "        else:\n",
        "            break\n",
        "    return truncated_comments\n",
        "\n",
        "def parse_comments(raw_comments):\n",
        "\n",
        "    comments = re.split(r'\\d+\\.\\s', raw_comments)\n",
        "    comments = [comment.strip() for comment in comments if comment.strip()]\n",
        "    return comments"
      ],
      "metadata": {
        "id": "fvINosvlmawZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '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'\n",
        "client = Groq(api_key=api_key)\n"
      ],
      "metadata": {
        "id": "8xLm0SkHmcnZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JOX1XUxcmeXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafb5e81-eb27-401b-8216-47fb5474130c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dailyhunt_dataset/Bangla_Reference_Summary.csv')\n",
        "news_text = list(df['Content'])\n",
        "comments = list(df['Comments'])"
      ],
      "metadata": {
        "id": "lIy6uu0pmgbY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''You are an AI assistant tasked with analyzing text and user comments to create concise summaries. Your summaries should integrate key information from all sources, maintaining the original language of the text. Focus on crucial points and minimize extraneous details.\n",
        "            The summary should be in the same language as that of the original news. Present only the summary, no additional commentary or captions or titles.'''\n"
      ],
      "metadata": {
        "id": "q-AiXroKmiAs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VJGDj3YKmipo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(news_text)), desc=\"Generating Headlines\"):\n",
        "    content = news_text[i]\n",
        "    raw_comments = comments[i]\n",
        "    reader_comments = parse_comments(raw_comments)\n",
        "    formatted_comments = ' '.join(reader_comments)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": 'system',\n",
        "            \"content\": f'''You are an AI assistant tasked with analyzing text and user comments to create concise summaries. Your summaries should integrate key information from all sources, maintaining the original language of the text. Focus on crucial points and minimize extraneous details.\n",
        "            The summary should be in the same language as that of the original news. Present only the summary, no additional commentary or captions or titles.\n",
        "            news: {content} comments: {formatted_comments}'''\n",
        "        }\n",
        "    ]\n",
        "    messages.append(draft_message(prompt))\n",
        "\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            temperature=1.0,\n",
        "            n=1,\n",
        "            model='llama3-8b-8192',\n",
        "            messages=messages\n",
        "        )\n",
        "        generated_summary = chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        if \"context_length_exceeded\" in error_message:\n",
        "            print(f\"Error generating headline for article {i+1}: Context length exceeded. Truncating content and comments.\")\n",
        "            truncated_content = truncate_content(content)\n",
        "            truncated_reader_comments = truncate_comments(reader_comments, max_length=480)\n",
        "            formatted_comments = ' '.join(truncated_reader_comments)\n",
        "            messages[0]['content'] = f'''news: {truncated_content} comments: {formatted_comments}'''\n",
        "            try:\n",
        "                chat_completion = client.chat.completions.create(\n",
        "                    temperature=1.0,\n",
        "                    n=1,\n",
        "                    model='llama3-8b-8192',\n",
        "                    messages=messages\n",
        "                )\n",
        "                generated_summary = chat_completion.choices[0].message.content\n",
        "            except Exception as e2:\n",
        "                print(f\"Error generating headline for article {i+1} after truncation: {e2}\")\n",
        "                generated_summary = \"Error generating summary\"\n",
        "        else:\n",
        "            print(f\"Error generating headline for article {i+1}: {e}\")\n",
        "            generated_summary = \"Error generating summary\"\n",
        "\n",
        "    generated_summaries.append(generated_summary)\n",
        "\n",
        "\n",
        "if len(generated_summaries) != len(df):\n",
        "    missing_entries = len(df) - len(generated_summaries)\n",
        "    generated_summaries.extend([\"Error generating summary\"] * missing_entries)\n",
        "\n",
        "df['Generated_summary_comments'] = generated_summaries\n",
        "df.to_csv('/content/drive/MyDrive/Dailyhunt_dataset/Bangla_Reference_Summary.csv', index=False)\n",
        "\n",
        "print(\"done sal!\")\n"
      ],
      "metadata": {
        "id": "R6SAFlaCmkfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168afcd1-f64b-40b8-b667-c24adef37491"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:   0%|          | 1/627 [00:00<09:55,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 2: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  10%|█         | 64/627 [06:50<1:03:55,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 65: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  17%|█▋        | 108/627 [12:08<1:09:12,  8.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 109: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  19%|█▉        | 121/627 [13:39<56:50,  6.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 122: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  20%|█▉        | 123/627 [13:53<52:58,  6.31s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 124: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  21%|██        | 131/627 [14:44<45:16,  5.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 132: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  27%|██▋       | 167/627 [18:25<54:20,  7.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 168: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  30%|██▉       | 186/627 [20:26<45:42,  6.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 187: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  51%|█████     | 320/627 [33:11<26:26,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 321: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  54%|█████▍    | 339/627 [35:15<30:25,  6.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 340: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  80%|████████  | 504/627 [53:19<11:12,  5.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 505: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines:  83%|████████▎ | 518/627 [54:52<12:19,  6.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating headline for article 519: Context length exceeded. Truncating content and comments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Headlines: 100%|██████████| 627/627 [1:07:22<00:00,  6.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done sal!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aXkrxtW5hsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}